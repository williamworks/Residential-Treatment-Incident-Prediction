{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Data in\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = 'data/fake_narratives.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna() #drops all the empty values because Excel is super awesome and not flawed in any way\n",
    "df['narrative'] = df['narrative'].str.strip()\n",
    "def normalize_initials(text):\n",
    "    return re.sub(r'\\b[A-Z]{2,3}\\b', '[CLIENT]', text)\n",
    "\n",
    "df['cleaned_narrative'] = df['narrative'].apply(normalize_initials)\n",
    "\n",
    "# df= df.sample(500, random_state=42) #safety sample for troubleshooting\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Embed the Narratives using a transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings = model.encode(df['cleaned_narrative'].tolist(), show_progress_bar=True)\n",
    "print(\"Embeddings took\", time.time() - start, \"seconds\")\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "print(\"starting TSNE\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_result = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Cluster the Embedded Narratives\n",
    "import hdbscan\n",
    "\n",
    "print(\"Starting Clustering\")\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean')\n",
    "cluster_labels = clusterer.fit_predict(tsne_result)\n",
    "df['cluster'] = cluster_labels\n",
    "print(\"\\nClustering took\", time.time() - start, \"seconds\")\n",
    "\n",
    "\n",
    "#Super-cluster the clusters\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "cluster_vectors = []\n",
    "cluster_ids = []\n",
    "\n",
    "for cluster_id in sorted(df['cluster'].unique()):\n",
    "    if cluster_id == -1:\n",
    "        continue\n",
    "    cluster_embs = embeddings[df['cluster'] == cluster_id]\n",
    "    cluster_vectors.append(np.mean(cluster_embs, axis=0))\n",
    "    cluster_ids.append(cluster_id)\n",
    "\n",
    "kmeans= KMeans(n_clusters=10, random_state=42)\n",
    "superclusters = kmeans.fit_predict(cluster_vectors)\n",
    "\n",
    "cluster_to_super = dict(zip(cluster_ids, superclusters))\n",
    "df['supercluster'] = df['cluster'].map(cluster_to_super)\n",
    "\n",
    "\n",
    "# # Develop the visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# Create your visualization DataFrame\n",
    "viz_df = pd.DataFrame({\n",
    "    'x': tsne_result[:, 0],\n",
    "    'y': tsne_result[:, 1],\n",
    "    'IndexScore': df['IndexScore'].values,\n",
    "    'supercluster': df['supercluster'].values\n",
    "})\n",
    "\n",
    "supercluster_stats = viz_df.groupby('supercluster')['IndexScore'].agg(['mean', 'count']).sort_values(by='mean', ascending=False)\n",
    "print(supercluster_stats.head(10))\n",
    "high_risk_clusters = supercluster_stats[supercluster_stats['mean'] > 5].index.tolist()\n",
    "\n",
    "\n",
    "# # Normalize colors and choose colormap\n",
    "norm = colors.Normalize(vmin=viz_df['IndexScore'].min(), vmax=viz_df['IndexScore'].max())\n",
    "cmap = cm.get_cmap('magma')\n",
    "\n",
    "# Create the figure + axes explicitly\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Scatter with Matplotlib directly (bypasses Seaborn's color limitations)\n",
    "sc = ax.scatter(\n",
    "    viz_df['x'],\n",
    "    viz_df['y'],\n",
    "    c=viz_df['IndexScore'],\n",
    "    cmap=cmap,\n",
    "    norm=norm,\n",
    "    s=20,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1],\n",
    "scatter = plt.scatter(\n",
    "    tsne_result[:, 0], \n",
    "    tsne_result[:, 1],\n",
    "    c=df['supercluster'],\n",
    "    cmap='nipy_spectral', \n",
    "    s=20,\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title('Narrative Embedding Superclusters')\n",
    "plt.xlabel('t-SNE-1'); plt.ylabel('t-SNE-2')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Add colorbar to the correct Axes\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.set_label(\"Index Score\")\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_title(\"Narrative Embeddings Colored by Index Score\")\n",
    "ax.set_xlabel(\"t-SNE Dimension 1\")\n",
    "ax.set_ylabel(\"t-SNE Dimension 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Printing out the narratives in full\n",
    "output_path = \"cluster_samples.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue  # Skip noise\n",
    "\n",
    "        subset = df[df['cluster'] == cluster_id]\n",
    "        n_to_sample = min(5, len(subset))\n",
    "\n",
    "        f.write(f\"\\n=== Cluster {cluster_id} ({len(subset)} narratives) ===\\n\")\n",
    "        for text in subset.sample(n=n_to_sample, random_state=42)['narrative']:\n",
    "            f.write(f\"- {text}\\n\")\n",
    "\n",
    "# LLM generated summaries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"  # or 't5-small' for performance\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=-1)\n",
    "\n",
    "output_path = \"supercluster_summaries.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for super_id in sorted(df['supercluster'].unique()):\n",
    "        if super_id == -1:\n",
    "            continue  # Skip noise\n",
    "\n",
    "        subset = df[df['supercluster'] == super_id]\n",
    "        sample_texts = subset['narrative'].dropna().sample(min(1000, len(subset)), random_state=42).tolist()\n",
    "        joined = \" \".join(sample_texts)\n",
    "        prompt = joined[:1024]  # truncate if needed for small models\n",
    "\n",
    "        # ðŸ”‘ TF-IDF Keyword Extraction\n",
    "        tfidf = TfidfVectorizer(stop_words='english', max_features=10)\n",
    "        if len(sample_texts) == 0 or len(\" \".join(sample_texts).split()) < 10:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            tfidf = TfidfVectorizer(stop_words='english', max_features=10)\n",
    "            tfidf.fit(sample_texts)\n",
    "            keywords = tfidf.get_feature_names_out()\n",
    "        except ValueError:\n",
    "            keywords = [\"[No keywords found]\"]\n",
    "\n",
    "        try:\n",
    "            summary = summarizer(prompt, max_length=50, min_length=10, do_sample=False)[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            summary = f\"[ERROR summarizing cluster {cluster_id}: {e}]\"\n",
    "\n",
    "        metrics = supercluster_stats.loc[super_id]\n",
    "        f.write(f\"\\n=== Cluster {super_id} ({len(subset)} narratives) ===\\n\")\n",
    "        f.write(f\"Avg IndexScore: {supercluster_stats['mean']:.2f}\\n\")\n",
    "        f.write(f\"Top Keywords: {', '.join(keywords)}\\n\")\n",
    "        f.write(f\"Summary: {summary}\\n\")\n",
    "        f.write(\"- Sample:\\n\")\n",
    "        for text in sample_texts:\n",
    "            f.write(f\"  â€¢ {text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
